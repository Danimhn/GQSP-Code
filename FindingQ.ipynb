{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNy74mKIKMm7rn9oCgEgLLN"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZQg6ZHARc6Z"
      },
      "outputs": [],
      "source": [
        "!pip install torch torch-complex"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# For Real Polynomials:"
      ],
      "metadata": {
        "id": "-7aYXGjQRpnF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn.functional import conv1d, pad\n",
        "from torch.fft import fft\n",
        "from torchaudio.transforms import FFTConvolve\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "# Use CUDA if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def objective_torch(x, P):\n",
        "    x.requires_grad = True\n",
        "\n",
        "    # Compute loss using squared distance function\n",
        "    loss = torch.norm(P - FFTConvolve(\"full\").forward(x, torch.flip(x, dims=[0])))**2\n",
        "    return loss\n",
        "\n",
        "times = []\n",
        "final_vals = []\n",
        "num_iterations = []\n",
        "\n",
        "for k in range(4, 20):\n",
        "    N = 2 ** k\n",
        "    poly = torch.randn(N, device=device)\n",
        "\n",
        "    granularity = 2 ** 25\n",
        "    P = pad(poly, (0, granularity - poly.shape[0]))\n",
        "    ft = fft(P)\n",
        "\n",
        "    # Normalize P\n",
        "    P_norms = torch.norm(ft, dim=-1)\n",
        "    poly /= torch.max(P_norms)\n",
        "\n",
        "    conv_p_negative = FFTConvolve(\"full\").forward(poly, torch.flip(poly, dims=[0]))* -1\n",
        "    conv_p_negative[poly.shape[0] - 1] = 1 - torch.norm(poly) ** 2\n",
        "\n",
        "    # Initializing Q randomly to start with\n",
        "    initial = torch.randn(poly.shape[0], device=device, requires_grad=True)\n",
        "    initial = (initial / torch.norm(initial)).clone().detach().requires_grad_(True)\n",
        "\n",
        "    optimizer = torch.optim.LBFGS([initial], max_iter=1000)\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    def closure():\n",
        "        optimizer.zero_grad()\n",
        "        loss = objective_torch(initial, conv_p_negative)\n",
        "        loss.backward()\n",
        "        return loss\n",
        "\n",
        "    optimizer.step(closure)\n",
        "\n",
        "    t1 = time.time()\n",
        "\n",
        "    total = t1-t0\n",
        "    times.append(total)\n",
        "    final_vals.append(closure().item())\n",
        "    num_iterations.append(optimizer.state[optimizer._params[0]]['n_iter'])\n",
        "    print(f'N: {N}')\n",
        "    print(f'Time: {total}')\n",
        "    print(f'Final: {closure().item()}')\n",
        "    print(f\"# Iterations: {optimizer.state[optimizer._params[0]]['n_iter']}\")\n",
        "    print(\"-----------------------------------------------------\")\n",
        "\n",
        "print(times)\n",
        "print(final_vals)\n",
        "print(num_iterations)\n"
      ],
      "metadata": {
        "id": "BMbFlmULRmUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# For Complex Polynomials:"
      ],
      "metadata": {
        "id": "k5XF3He2RxBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn.functional import conv1d, pad\n",
        "from torch.fft import fft\n",
        "from torchaudio.transforms import Convolve, FFTConvolve\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "# Use CUDA if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def objective_torch(x, P):\n",
        "    x.requires_grad = True\n",
        "\n",
        "    real_part = x[:len(x) // 2]\n",
        "    imag_part = x[len(x) // 2:]\n",
        "\n",
        "    real_flip = torch.flip(real_part, dims=[0])\n",
        "    imag_flip = torch.flip(-1*imag_part, dims=[0])\n",
        "\n",
        "    conv_real_part = FFTConvolve(\"full\").forward(real_part, real_flip)\n",
        "    conv_imag_part = FFTConvolve(\"full\").forward(imag_part, imag_flip)\n",
        "\n",
        "    conv_real_imag = FFTConvolve(\"full\").forward(real_part, imag_flip)\n",
        "    conv_imag_real = FFTConvolve(\"full\").forward(imag_part, real_flip)\n",
        "\n",
        "    # Compute real and imaginary part of the convolution\n",
        "    real_conv = conv_real_part - conv_imag_part\n",
        "    imag_conv = conv_real_imag + conv_imag_real\n",
        "\n",
        "    # Combine to form the complex result\n",
        "    conv_result = torch.complex(real_conv, imag_conv)\n",
        "\n",
        "    # Compute loss using squared distance function\n",
        "    loss = torch.norm(P - conv_result)**2\n",
        "    return loss\n",
        "\n",
        "def complex_conv_by_flip_conj(x):\n",
        "    real_part = x.real\n",
        "    imag_part = x.imag\n",
        "\n",
        "    real_flip = torch.flip(real_part, dims=[0])\n",
        "    imag_flip = torch.flip(-1*imag_part, dims=[0])\n",
        "\n",
        "    conv_real_part = FFTConvolve(\"full\").forward(real_part, real_flip)\n",
        "    conv_imag_part = FFTConvolve(\"full\").forward(imag_part, imag_flip)\n",
        "\n",
        "    conv_real_imag = FFTConvolve(\"full\").forward(real_part, imag_flip)\n",
        "    conv_imag_real = FFTConvolve(\"full\").forward(imag_part, real_flip)\n",
        "\n",
        "    # Compute real and imaginary part of the convolution\n",
        "    real_conv = conv_real_part - conv_imag_part\n",
        "    imag_conv = conv_real_imag + conv_imag_real\n",
        "\n",
        "    # Combine to form the complex result\n",
        "    return torch.complex(real_conv, imag_conv)\n",
        "\n",
        "times = []\n",
        "final_vals = []\n",
        "num_iterations = []\n",
        "\n",
        "for k in range(4, 20):\n",
        "    N = 2 ** k\n",
        "    poly = torch.randn(N, device=device) + 1.j * torch.randn(N, device=device)\n",
        "\n",
        "    granularity = 2 ** 25\n",
        "    P = pad(poly, (0, granularity - poly.shape[0]))\n",
        "    ft = fft(P)\n",
        "\n",
        "    # Normalize P\n",
        "    P_norms = torch.norm(ft, dim=-1)\n",
        "    poly /= torch.max(P_norms)\n",
        "\n",
        "    conv_p_negative = complex_conv_by_flip_conj(poly)*-1\n",
        "    conv_p_negative[poly.shape[0] - 1] = 1 - torch.norm(poly) ** 2\n",
        "\n",
        "    # Initializing Q randomly to start with\n",
        "    initial = torch.randn(poly.shape[0]*2, device=device, requires_grad=True)\n",
        "    initial = (initial / torch.norm(initial)).clone().detach().requires_grad_(True)\n",
        "\n",
        "    optimizer = torch.optim.LBFGS([initial], max_iter=1000)\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    def closure():\n",
        "        optimizer.zero_grad()\n",
        "        loss = objective_torch(initial, conv_p_negative)\n",
        "        loss.backward()\n",
        "        return loss\n",
        "\n",
        "    optimizer.step(closure)\n",
        "\n",
        "    t1 = time.time()\n",
        "\n",
        "    total = t1-t0\n",
        "    times.append(total)\n",
        "    final_vals.append(closure().item())\n",
        "    num_iterations.append(optimizer.state[optimizer._params[0]]['n_iter'])\n",
        "    print(f'N: {N}')\n",
        "    print(f'Time: {total}')\n",
        "    print(f'Final: {closure().item()}')\n",
        "    print(f\"# Iterations: {optimizer.state[optimizer._params[0]]['n_iter']}\")\n",
        "    print(\"-----------------------------------------------------\")\n",
        "\n",
        "print(times)\n",
        "print(final_vals)\n",
        "print(num_iterations)\n"
      ],
      "metadata": {
        "id": "HMt6BYmERzhe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}